{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EchoSphere","text":"<p>EchoSphere is a fast, lightweight SQL testing framework designed to validate your data workflows with simple, readable SQL files. It focuses on developer experience and velocity: write tests as SQL, run them in parallel, and integrate the results into your CI/CD.</p> <p>EchoSphere derives from the mythological nymph Echo \u2014 a symbol of reflection and resonance. Your tests \u201cecho\u201d the health of your data systems.</p>"},{"location":"#runs-your-tests-on","title":"Runs your tests on \ud83d\udd2c","text":""},{"location":"#why-echosphere","title":"Why EchoSphere? \u26a1","text":"<ul> <li>Simple: Tests are plain SQL files with a <code>.es.sql</code> suffix</li> <li>Fast: Concurrent execution for quick feedback</li> <li>CI-ready: JUnit output, clear exit codes, and machine-readable results</li> <li>Visibility: Export data issues into local Excel files</li> </ul>"},{"location":"#quick-links","title":"Quick Links \ud83d\udd17","text":"<ul> <li>Getting Started: installation, first setup, and a 5\u2011minute quickstart</li> <li>User Guide: workflows, environment management, and writing effective tests</li> <li>Command Reference: full CLI documentation for <code>es</code></li> <li>Advanced Topics: CI/CD, performance, and extensions</li> <li>Troubleshooting: common issues and debugging tips</li> </ul>"},{"location":"#what-a-test-looks-like","title":"What a Test Looks Like \ud83e\uddea","text":"<p>EchoSphere considers a test successful if it returns zero rows. If the query returns one or more rows, the test fails and the rows explain what was wrong.</p> <pre><code>-- file: tests/orders_total.es.sql\nSELECT *\nFROM (\n  SELECT SUM(O_TOTALPRICE) AS total\n  FROM ORDERS\n  WHERE O_ORDERDATE = '1995-02-19'\n)\nWHERE total &lt;&gt; 944870465.07;\n</code></pre>"},{"location":"#latest-version-and-changelog","title":"Latest Version and Changelog \ud83d\udcdd","text":"<ul> <li>Versioning aligns with project releases on GitHub.</li> <li>See the repository releases tab for changelog highlights.</li> </ul>"},{"location":"contributing/","title":"Contributing to EchoSphere Docs","text":"<p>We welcome improvements to the documentation. This guide explains how to contribute and how docs are maintained.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the repository and create a feature branch.</li> <li>Edit Markdown files under <code>docs/</code>.</li> <li>Preview locally with MkDocs Material:    <code>sh    pip install mkdocs mkdocs-material    mkdocs serve</code></li> <li>Submit a pull request with a clear description and screenshots if visual changes are significant.</li> </ol>"},{"location":"contributing/#style-guide","title":"Style Guide","text":"<ul> <li>Be concise and task\u2011oriented; prefer examples over theory.</li> <li>Use sentence case for headings.</li> <li>Use admonitions for notes, tips, and warnings.</li> <li>Keep code blocks copy\u2011paste ready.</li> </ul>"},{"location":"contributing/#structure","title":"Structure","text":"<ul> <li>Getting Started \u2192 quick paths for new users</li> <li>User Guide \u2192 workflows and how\u2011tos</li> <li>Command Reference \u2192 precise CLI options</li> <li>Reference \u2192 configuration and environment variables</li> <li>Advanced \u2192 CI/CD, performance, scale</li> <li>Examples/Tutorials \u2192 cookbook and end\u2011to\u2011end guides</li> <li>Troubleshooting \u2192 common issues and resolutions</li> </ul>"},{"location":"contributing/#maintenance-plan","title":"Maintenance Plan","text":"<ul> <li>Review high\u2011traffic pages monthly; full sweep quarterly.</li> <li>Keep docs in sync with releases; update command and option references.</li> <li>CI should build docs and check links (planned).</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive. See the repository\u2019s main guidelines for behavior and reporting.</p>"},{"location":"advanced/ci-cd/","title":"CI/CD Integration","text":"<p>Integrate EchoSphere into your CI pipelines to continuously validate data quality.</p>"},{"location":"advanced/ci-cd/#github-actions","title":"GitHub Actions","text":"<p>Example workflow that installs dependencies and runs tests on every push:</p> <pre><code>name: echo-sphere-tests\non:\n  push:\n    branches: [ main ]\n  pull_request:\n\njobs:\n  tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - name: Install EchoSphere\n        run: |\n          pip install --upgrade pip\n          pip install .\n      - name: Configure environment\n        env:\n          ES_ENV_NAME: dev\n          # Provide Snowflake secrets via GitHub Actions Secrets\n          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}\n          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n        run: |\n          # Template your es.ini from a secure location or use env variables\n          echo \"[default]\" &gt; es.ini\n          echo \"env = env.snowflake.dev\" &gt;&gt; es.ini\n          echo \"[env.snowflake.dev]\" &gt;&gt; es.ini\n          echo \"platform = snowflake\" &gt;&gt; es.ini\n          echo \"user = ${SNOWFLAKE_USER}\" &gt;&gt; es.ini\n          echo \"password = ${SNOWFLAKE_PASSWORD}\" &gt;&gt; es.ini\n          echo \"account = ${{ secrets.SNOWFLAKE_ACCOUNT }}\" &gt;&gt; es.ini\n          echo \"warehouse = ${{ secrets.SNOWFLAKE_WAREHOUSE }}\" &gt;&gt; es.ini\n          echo \"role = ${{ secrets.SNOWFLAKE_ROLE }}\" &gt;&gt; es.ini\n          echo \"database = ${{ secrets.SNOWFLAKE_DATABASE }}\" &gt;&gt; es.ini\n          echo \"schema = ${{ secrets.SNOWFLAKE_SCHEMA }}\" &gt;&gt; es.ini\n      - name: Run EchoSphere\n        run: |\n          es run --junitxml reports/junit.xml || true\n      - name: Publish Test Report\n        if: always()\n        uses: mikepenz/action-junit-report@v4\n        with:\n          report_paths: 'reports/junit.xml'\n</code></pre>"},{"location":"advanced/ci-cd/#jenkins-pipeline","title":"Jenkins Pipeline","text":"<pre><code>pipeline {\n  agent any\n  stages {\n    stage('Setup') {\n      steps {\n        sh 'pip install .'\n      }\n    }\n    stage('Configure') {\n      steps {\n        sh '''\n        cat &gt; es.ini &lt;&lt;EOF\n        [default]\n        env = env.snowflake.dev\n        [env.snowflake.dev]\n        platform = snowflake\n        user = $SNOWFLAKE_USER\n        password = $SNOWFLAKE_PASSWORD\n        account = $SNOWFLAKE_ACCOUNT\n        warehouse = $SNOWFLAKE_WAREHOUSE\n        role = $SNOWFLAKE_ROLE\n        database = $SNOWFLAKE_DATABASE\n        schema = $SNOWFLAKE_SCHEMA\n        EOF\n        '''\n      }\n    }\n    stage('Test') {\n      steps {\n        sh 'es run --junitxml reports/junit.xml'\n      }\n      post {\n        always {\n          junit 'reports/junit.xml'\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"advanced/ci-cd/#tips","title":"Tips","text":"<ul> <li>Examples above use Snowflake for brevity; Postgres is supported as well. Adapt <code>es.ini</code> stanzas accordingly and install the appropriate extra (<code>EchoSphere[postgres]</code>).</li> <li>Use <code>--export-failures</code> to attach detailed artifacts for debugging</li> <li>Fail the build when EchoSphere exits non\u2011zero, or allow failures but surface reports depending on your gate policy</li> <li>Keep secrets out of the repo; use CI secret stores</li> </ul>"},{"location":"advanced/large-scale/","title":"Large-Scale Test Management","text":"<p>Strategies and patterns for managing thousands of SQL tests across teams.</p>"},{"location":"advanced/large-scale/#organization","title":"Organization","text":"<ul> <li>Group tests by domain (customer, orders, finance) and by frequency (smoke, daily, weekly).</li> <li>Use consistent naming and folder conventions to make ownership clear.</li> </ul>"},{"location":"advanced/large-scale/#execution-planning","title":"Execution Planning","text":"<ul> <li>Run fast smoke suites on every commit; run heavier integrity suites nightly.</li> <li>Shard large suites by subsuite across CI jobs to keep wall-clock low.</li> </ul>"},{"location":"advanced/large-scale/#governance","title":"Governance","text":"<ul> <li>Require code reviews for test additions/changes.</li> <li>Track flaky tests and quarantine with a plan to fix.</li> </ul>"},{"location":"advanced/large-scale/#reporting","title":"Reporting","text":"<ul> <li>Publish JUnit XML to your CI test report UI.</li> <li>Export failing rows to Excel for stakeholders who prefer spreadsheets.</li> </ul>"},{"location":"advanced/performance/","title":"Performance Optimization","text":"<p>EchoSphere is designed to execute tests concurrently. Use the tips below to keep runs fast and stable.</p>"},{"location":"advanced/performance/#parallel-execution","title":"Parallel Execution","text":"<ul> <li>EchoSphere runs tests in parallel to reduce total runtime.</li> <li>Keep individual tests lightweight; avoid full table scans when possible.</li> <li>Partition large validations by date or key ranges.</li> </ul>"},{"location":"advanced/performance/#query-tuning","title":"Query Tuning","text":"<ul> <li>Filter early and select only needed columns.</li> <li>Use appropriate clustering/partitioning in Snowflake to improve aggregation and filter performance.</li> <li>Consider pre-aggregations for expensive checks and validate the aggregates instead of raw detail.</li> </ul>"},{"location":"advanced/performance/#managing-resources","title":"Managing Resources","text":"<ul> <li>Choose a warehouse size appropriate for your workload.</li> <li>Run heavy suites during off-peak hours where possible.</li> <li>Use separate warehouses for CI to avoid contention with production workloads.</li> </ul>"},{"location":"advanced/performance/#flaky-tests","title":"Flaky Tests","text":"<ul> <li>Avoid nondeterministic functions unless inputs are fixed.</li> <li>Control time-based logic by fixing a date window (e.g., yesterday) instead of <code>CURRENT_DATE</code> when appropriate.</li> </ul>"},{"location":"advanced/performance/#measuring","title":"Measuring","text":"<ul> <li>Track duration of suites across runs (your CI can record job duration).</li> <li>Export failing rows to analyze patterns of slowness in specific checks.</li> </ul>"},{"location":"command-reference/","title":"Command Reference","text":"<p>This section documents all EchoSphere CLI commands, their options, and examples.</p> <p>EchoSphere uses the <code>es</code> command with subcommands.</p>"},{"location":"command-reference/#global-behavior","title":"Global Behavior","text":"<ul> <li><code>-h</code>, <code>--help</code> shows help for any command.</li> <li>Exit code 0: success, non\u2011zero: failure.</li> <li>Some options accept environment variables for convenience.</li> </ul>"},{"location":"command-reference/#es-setup","title":"es setup","text":"<p>Initialize EchoSphere for your platform and scaffold a suite and configuration.</p> <p>Usage:</p> <pre><code>es setup --platform SNOWFLAKE  # or POSTGRES\n</code></pre> <ul> <li>--platform PLATFORM</li> <li>Required. Select the target platform to configure. Supported: <code>SNOWFLAKE</code>, <code>POSTGRES</code>.</li> </ul> <p>What it does: - Creates a default tests directory - Generates a configuration file (<code>es.ini</code>) with environment stanzas</p>"},{"location":"command-reference/#es-run","title":"es run","text":"<p>Run all discovered tests concurrently.</p> <p>Usage:</p> <pre><code># choose environment via CLI\nes run --environment dev\n\n# or via environment variable\nES_ENV_NAME=dev es run\n\n# export JUnit XML and failed rows to Excel\nes run -e dev --junitxml reports/junit.xml --export-failures reports/failures.xlsx\n</code></pre> <p>Options: - -e, --environment NAME   - Select the environment to run against.   - Environment variable: <code>ES_ENV_NAME</code> (if set, you may omit <code>-e</code>). - --junitxml PATH   - Write JUnit XML results to PATH (directories will be created if missing). - --export-failures PATH   - Write an Excel (.xlsx) with failing test result rows to PATH (directories will be created if missing).   - Captures up to 1000 rows per failed test (including column headers). May increase query time and warehouse/DB cost.</p> <p>Behavior: - Discovers tests with the <code>.es.sql</code> suffix - A test passes if the executed SQL returns zero rows - Runs tests concurrently and prints a summary - Non\u2011zero exit on any failure</p>"},{"location":"command-reference/#es-view","title":"es view","text":"<p>Explore your suite: list tests or display the SQL code of a single test.</p>"},{"location":"command-reference/#es-view-tests","title":"es view tests","text":"<p>List the test suite. You can show all tests or filter by subsuite.</p> <pre><code># list all tests\nes view tests --all\n\n# filter by subsuite (subdirectory)\nes view tests --suite smoke\n</code></pre> <p>Options: - -a, --all   - Show all tests regardless of subsuite. - -s, --suite NAME   - Filter tests by subsuite. Cannot be used together with <code>--all</code>.</p>"},{"location":"command-reference/#es-view-test","title":"es view test","text":"<p>Print the SQL code for a given test.</p> <pre><code># show SQL for a test named orders_total.es.sql in the root suite\nes view test orders_total\n\n# or when inside a subsuite\nes view test smoke/orders_total\n</code></pre> <p>Parameters: - name: The test identifier, optionally including subsuite as <code>&lt;subsuite&gt;/&lt;test_name&gt;</code>.</p>"},{"location":"examples/cookbook/","title":"Cookbook","text":"<p>Common recipes you can copy\u2011paste and adapt to your project.</p>"},{"location":"examples/cookbook/#export-reports-in-ci","title":"Export Reports in CI","text":"<pre><code>es run -e dev --junitxml reports/junit.xml --export-failures reports/failures.xlsx\n</code></pre>"},{"location":"examples/cookbook/#run-a-subsuite-only","title":"Run a Subsuite Only","text":"<p>Organize tests into subdirectories and filter using <code>es view</code> for discovery, or simply commit only the subsuite you want to run in a branch.</p> <p>Listing a subsuite:</p> <pre><code>es view tests --suite smoke\n</code></pre>"},{"location":"examples/cookbook/#investigate-a-single-test","title":"Investigate a Single Test","text":"<pre><code>es view test integrity/no_duplicate_orders\n</code></pre>"},{"location":"examples/cookbook/#create-a-new-project","title":"Create a New Project","text":"<pre><code>pip install git+https://github.com/MauriceKuenicke/EchoSphere\nes setup --platform SNOWFLAKE  # or POSTGRES\n</code></pre>"},{"location":"examples/cookbook/#example-assertion-templates","title":"Example Assertion Templates","text":"<ul> <li>No NULLs in key column</li> </ul> <pre><code>SELECT KEY\nFROM MY_TABLE\nWHERE KEY IS NULL;\n</code></pre> <ul> <li>No duplicates in key</li> </ul> <pre><code>SELECT KEY\nFROM MY_TABLE\nGROUP BY KEY\nHAVING COUNT(*) &gt; 1;\n</code></pre> <ul> <li>Aggregate value equals expectation</li> </ul> <pre><code>SELECT *\nFROM (\n  SELECT SUM(AMOUNT) AS total\n  FROM MY_TABLE\n  WHERE DATE_COL = CURRENT_DATE - INTERVAL '1 DAY'\n)\nWHERE total &lt;&gt; 123.45;\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section helps you install EchoSphere, perform the first-time setup, and run a \u201cHello World\u201d test in minutes.</p> <ul> <li>Installation</li> <li>Initial configuration</li> <li>Hello World: first test</li> <li>Key concepts</li> <li>5\u2011minute quickstart</li> </ul>"},{"location":"getting-started/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/#using-pip-recommended","title":"Using pip (recommended)","text":"<pre><code># Install from PyPI with platform extras:\n# Snowflake:\npip install EchoSphere[snowflake]\n# Postgres:\npip install EchoSphere[postgres]\n\n# Alternatively, install latest from GitHub:\npip install git+https://github.com/MauriceKuenicke/EchoSphere\n</code></pre>"},{"location":"getting-started/#from-source-development","title":"From source (development)","text":"<pre><code># clone the repo\ngit clone https://github.com/MauriceKuenicke/EchoSphere\ncd EchoSphere\n\n# install in editable mode\npip install -e .[dev]\n</code></pre>"},{"location":"getting-started/#initial-configuration","title":"Initial Configuration","text":"<p>After installation, run the setup command to scaffold a test suite and configuration:</p> <pre><code>es setup --platform SNOWFLAKE  # or POSTGRES\n</code></pre> <p>This will: - create a default folder for your SQL tests - create a configuration file for your platform credentials and environments</p> <p>Note: If you omit the platform, EchoSphere will ask for one. Use <code>--help</code> to list platforms.</p>"},{"location":"getting-started/#hello-world-first-test","title":"Hello World: First Test","text":"<ol> <li>Create a file named <code>example.es.sql</code> in your tests directory.</li> <li>Paste a simple query that fails only when your expectation is violated:</li> </ol> <pre><code>SELECT 1 WHERE 1 &lt;&gt; 1;  -- returns zero rows, therefore passes\n</code></pre> <ol> <li>Run the test suite:</li> </ol> <pre><code>es run --environment dev\n</code></pre> <ul> <li>Success: exit code 0, no failing tests</li> <li>Failure: non\u2011zero exit code; failed tests are reported with their returned rows</li> </ul> <p>Tip: You can export results to JUnit XML and export failing rows to Excel. See the Command Reference for details.</p>"},{"location":"getting-started/#key-concepts","title":"Key Concepts","text":"<ul> <li>Test success: a test passes if the SQL returns zero rows.</li> <li>Naming: use the <code>.es.sql</code> suffix so EchoSphere can discover your tests.</li> <li>Environments: define credentials and defaults in <code>es.ini</code> for your platform; select at runtime with <code>--environment</code> or the <code>ES_ENV_NAME</code> env var.</li> <li>Parallelism: tests are executed concurrently to minimize runtime.</li> </ul>"},{"location":"getting-started/#5minute-quickstart","title":"5\u2011Minute Quickstart","text":"<ol> <li>Install EchoSphere</li> <li>Run <code>es setup --platform SNOWFLAKE</code> or <code>es setup --platform POSTGRES</code></li> <li>Configure your platform credentials in <code>es.ini</code></li> <li>Add <code>orders_total.es.sql</code> with your first assertion query</li> <li>Run <code>es run -e dev</code> and inspect the output</li> <li>Optional: <code>es run -e dev --junitxml reports/junit.xml --export-failures reports/failures.xlsx</code></li> </ol>"},{"location":"reference/api-usage/","title":"API Usage (Programmatic)","text":"<p>EchoSphere is primarily a CLI tool. While internal modules exist, the public, supported interface is the <code>es</code> command. Programmatic use is not considered stable and may change without notice.</p> <p>That said, advanced users can embed the CLI using Typer\u2019s application object.</p>"},{"location":"reference/api-usage/#invoke-the-cli-from-python","title":"Invoke the CLI from Python","text":"<pre><code>from echosphere.main import app\n\n# This will run the Typer application, similar to invoking `es` on the command line\n# Be cautious: this will call sys.exit() on failures just like the CLI does.\nif __name__ == \"__main__\":\n    app()\n</code></pre>"},{"location":"reference/api-usage/#recommended-approach","title":"Recommended Approach","text":"<ul> <li>Prefer invoking <code>es</code> via subprocess in automation scripts.</li> <li>Use <code>--junitxml</code> to generate machine-readable results for CI systems.</li> <li>Use <code>--export-failures</code> to capture failing rows for post-processing (captures up to 1000 rows per failed test, including column headers).</li> </ul> <p>Example (Python):</p> <pre><code>import subprocess\n\nresult = subprocess.run([\n    \"es\", \"run\", \"-e\", \"dev\",\n    \"--junitxml\", \"reports/junit.xml\",\n], capture_output=True, text=True)\n\nprint(result.stdout)\nprint(result.stderr)\nprint(result.returncode)  # 0 = success\n</code></pre> <p>If you need a stabilized programmatic API in the future, please open a feature request describing your use case.</p>"},{"location":"reference/configuration/","title":"Configuration Reference","text":"<p>EchoSphere reads environment configuration from an <code>es.ini</code> file. This file defines one or more environments and a default.</p>"},{"location":"reference/configuration/#file-structure","title":"File Structure","text":"<pre><code>[default]\nenv = env.snowflake.dev  # Name of the default environment (could also be env.postgres.dev)\n\n[env.snowflake.dev]\nplatform = snowflake\nuser = ...\npassword = ...\naccount = ...\nwarehouse = ...\nrole = ...\ndatabase = ...\nschema = ...\n\n[env.snowflake.prod]\nplatform = snowflake\nuser = ...\npassword = ...\naccount = ...\nwarehouse = ...\nrole = ...\ndatabase = ...\nschema = ...\n\n[env.postgres.dev]\nplatform = postgres\nhost = ...\nport = 5432\ndatabase = ...\nschema = public\nuser = ...\npassword = ...\nsslmode = ...  # optional\n</code></pre>"},{"location":"reference/configuration/#sections","title":"Sections","text":"<ul> <li>[default]</li> <li><code>env</code>: the environment name to use when no <code>--environment</code> is provided.</li> <li>[env..] <li><code>platform</code>: <code>snowflake</code> or <code>postgres</code>.</li> <li>For Snowflake: <code>user</code>, <code>password</code>, <code>account</code>, <code>warehouse</code>, <code>role</code>, <code>database</code>, <code>schema</code>.</li> <li>For Postgres: <code>host</code>, <code>port</code>, <code>database</code>, <code>schema</code>, <code>user</code>, <code>password</code>, <code>sslmode</code> (optional).</li>"},{"location":"reference/configuration/#selecting-the-environment","title":"Selecting the Environment","text":"<ul> <li>CLI option: <code>es run --environment dev</code></li> <li>Environment variable: <code>ES_ENV_NAME=dev es run</code></li> </ul> <p>If neither is provided, EchoSphere uses the environment defined in <code>[default]</code>.</p>"},{"location":"reference/configuration/#test-discovery","title":"Test Discovery","text":"<ul> <li>EchoSphere discovers tests with the <code>.es.sql</code> suffix.</li> <li>Organize tests into subdirectories for logical grouping; all are discovered recursively.</li> </ul>"},{"location":"reference/configuration/#runtime-options","title":"Runtime Options","text":"<ul> <li>Parallel execution is enabled by default to speed up test runs.</li> <li>Export options:</li> <li><code>--junitxml PATH</code> \u2014 write JUnit XML report</li> <li><code>--export-failures PATH</code> \u2014 write failing rows to Excel (.xlsx)</li> </ul>"},{"location":"reference/configuration/#best-practices-for-configuration","title":"Best Practices for Configuration","text":"<ul> <li>Avoid hardcoding secrets in <code>es.ini</code>. Use your secret store or CI secrets to template values at runtime.</li> <li>Use separate environments for dev/staging/prod with least privilege.</li> <li>Keep <code>database</code> and <code>schema</code> values environment-specific for flexibility.</li> </ul>"},{"location":"reference/environment-variables/","title":"Environment Variables","text":"<p>You can control EchoSphere behavior via environment variables.</p> <ul> <li>ES_ENV_NAME</li> <li>Selects the target environment when running tests.</li> <li>Equivalent to passing <code>--environment</code> on the CLI.</li> <li>Example:     <code>sh     ES_ENV_NAME=dev es run</code></li> </ul> <p>Additional environment variables may be introduced in future releases as features evolve. Prefer the CLI options when available for clarity in scripts.</p>"},{"location":"reference/sql-testing-syntax/","title":"SQL Testing Syntax (EchoSphere)","text":"<p>EchoSphere follows a simple, explicit convention:</p> <ul> <li>A test is any file ending with <code>.es.sql</code>.</li> <li>A test PASSES when the query returns zero rows.</li> <li>A test FAILS when the query returns one or more rows \u2014 those rows explain the failure.</li> </ul> <p>This design keeps tests readable and leverages the full expressiveness of SQL.</p>"},{"location":"reference/sql-testing-syntax/#patterns","title":"Patterns","text":""},{"location":"reference/sql-testing-syntax/#assert-equality","title":"Assert Equality","text":"<p>Return a row only when the value differs.</p> <pre><code>SELECT *\nFROM (\n  SELECT SUM(AMOUNT) AS total\n  FROM PAYMENTS\n  WHERE DATE = CURRENT_DATE - 1\n)\nWHERE total &lt;&gt; 123.45;\n</code></pre>"},{"location":"reference/sql-testing-syntax/#assert-nonnull","title":"Assert Non\u2011Null","text":"<pre><code>SELECT CUSTOMER_ID\nFROM CUSTOMERS\nWHERE CUSTOMER_ID IS NULL;\n</code></pre>"},{"location":"reference/sql-testing-syntax/#assert-uniqueness","title":"Assert Uniqueness","text":"<pre><code>SELECT ORDER_ID\nFROM ORDERS\nGROUP BY ORDER_ID\nHAVING COUNT(*) &gt; 1;\n</code></pre>"},{"location":"reference/sql-testing-syntax/#conditionalscoped-checks","title":"Conditional/Scoped Checks","text":"<p>Use WHERE clauses to scope checks to recent partitions or specific segments.</p> <pre><code>SELECT *\nFROM (\n  SELECT COUNT(*) AS cnt\n  FROM EVENTS\n  WHERE EVENT_DATE &gt;= CURRENT_DATE - 7\n)\nWHERE cnt = 0;  -- Fail if no events in the last 7 days\n</code></pre>"},{"location":"reference/sql-testing-syntax/#recommendations","title":"Recommendations","text":"<ul> <li>Keep each test focused (one assertion per file when possible).</li> <li>Avoid cross\u2011environment fully qualified names; rely on environment config for DB/SCHEMA.</li> <li>Prefer deterministic logic; control time windows to reduce flakes.</li> <li>Add comments at the top explaining the business rule and what a failure row means.</li> </ul>"},{"location":"supported-platforms/","title":"Supported Platforms","text":"<p>This page provides an overview of the platforms  you can use to execute SQL-based tests and the necessary  configuration values required for the setup.</p>"},{"location":"supported-platforms/#supported","title":"Supported","text":"<ol> <li> PostgreSQL \u2705</li> <li> Snowflake \u2705</li> <li> Databricks \u2705</li> </ol>"},{"location":"supported-platforms/#in-planning","title":"In Planning","text":"<ol> <li>Firebolt \u274c</li> <li>AWS Redshift \u274c</li> <li>Azure Synapse \u274c</li> <li>Google BigQuery \u274c</li> <li>MS SQL Server \u274c</li> </ol>"},{"location":"supported-platforms/#postgresql","title":"PostgreSQL","text":"<p>PostgreSQL is a powerful, open-source relational database system.:</p> <p>Configuration Values: - <code>host</code>: The hostname or IP address of your PostgreSQL server. - <code>port</code>: The port on which PostgreSQL is running (default is <code>5432</code>). - <code>dbname</code>: The name of the database to connect to. - <code>user</code>: The username for authentication. - <code>password</code>: The password for the user. - <code>schema</code>: The schema to set in the current connection session. - <code>sslmode</code> (optional): The SSL mode for the connection (e.g., <code>disable</code>, <code>allow</code>, <code>prefer</code>, <code>require</code>).</p> <p>Example Configuration File:</p> <pre><code>[DEFAULT]\nhost = your-postgresql-host\nport = 5432\ndbname = your-database-name\nuser = your-username\npassword = your-password\nschema = your-schema-name\nsslmode = disable\n</code></pre>"},{"location":"supported-platforms/#snowflake","title":"Snowflake","text":"<p>Snowflake is a fully managed cloud data platform that supports scalable storage and high-performance analytics. Here is how to configure Snowflake for executing your SQL-based tests.</p> <p>Configuration Values: - <code>account</code>: Your Snowflake account identifier  - <code>user</code>: The username for Snowflake authentication. - <code>password</code>: The password for your Snowflake account. - <code>warehouse</code>: The name of the virtual warehouse to use for computations. - <code>database</code>: The name of the database to connect to. - <code>schema</code>: The schema to use within the specified database. - <code>role</code> (optional): The role to assume for the session (default is the user's default role).</p> <p>Example Configuration File:</p> <pre><code>[DEFAULT]\naccount = your-snowflake-account\nuser = your-snowflake-user\npassword = your-snowflake-password\nwarehouse = your-virtual-warehouse\ndatabase = your-database\nschema = your-schema-name\nrole = your-role-name\n</code></pre>"},{"location":"supported-platforms/#databricks","title":"Databricks","text":"<p>Databricks provides a cloud-based data analytics platform and SQL-based querying engine.:</p> <p>Configuration Values: - <code>server_hostname</code>: The Databricks server's hostname. - <code>http_path</code>: The HTTP Path of the Databricks SQL endpoint. - <code>access_token</code>: Your personal access token for authentication.</p> <p>Example Configuration File:</p> <pre><code>[DEFAULT]\nserver_hostname = your-databricks-hostname\nhttp_path = your-databricks-http-path\naccess_token = your-access-token\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Having issues? This guide helps you diagnose and fix common problems.</p>"},{"location":"troubleshooting/#connection-problems","title":"Connection Problems","text":"<ul> <li>Symptom: authentication or network errors.</li> <li>Checks:</li> <li>Verify <code>es.ini</code> credentials for the selected environment.</li> <li>For Snowflake: ensure account, role, warehouse, database, and schema are correct; confirm network access to Snowflake from your environment/CI.</li> <li>For Postgres: ensure host, port, database, schema, user, and password are correct; confirm network/firewall access to the Postgres server and set an appropriate <code>sslmode</code> (if required).</li> </ul>"},{"location":"troubleshooting/#no-tests-discovered","title":"No Tests Discovered","text":"<ul> <li>Symptom: EchoSphere reports zero tests executed.</li> <li>Checks:</li> <li>Ensure test files end with <code>.es.sql</code>.</li> <li>Verify your tests directory path and that files are readable.</li> <li>Run <code>es view tests --all</code> to see what EchoSphere detects.</li> </ul>"},{"location":"troubleshooting/#failing-tests-without-clear-cause","title":"Failing Tests Without Clear Cause","text":"<ul> <li>Use <code>es view test &lt;name&gt;</code> to print the SQL and review logic.</li> <li>Inspect the failing rows (export with <code>--export-failures</code> for deeper analysis).</li> <li>Temporarily narrow the scope (e.g., one partition/day) to isolate the issue.</li> </ul>"},{"location":"troubleshooting/#slow-test-runs","title":"Slow Test Runs","text":"<ul> <li>Check Performance Optimization for tips (parallelism, query tuning, resource sizing).</li> <li>Split very heavy checks into smaller targeted tests.</li> </ul>"},{"location":"troubleshooting/#ci-fails-but-works-locally","title":"CI Fails but Works Locally","text":"<ul> <li>Ensure <code>es.ini</code> is templated correctly in CI and secrets are passed.</li> <li>Match Python versions and dependency versions between local and CI.</li> <li>Publish JUnit XML and view CI test logs for details.</li> </ul>"},{"location":"troubleshooting/#when-to-file-a-bug-report","title":"When to File a Bug Report","text":"<ul> <li>Provide EchoSphere version, platform details, minimal reproduction (test SQL), and logs.</li> <li>Open an issue on GitHub with all relevant context and redacted credentials.</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>This section covers day-to-day workflows, environment setup, writing effective tests, and how to run and read results.</p> <ul> <li>Commands overview</li> <li>Environment configuration</li> <li>Writing tests</li> <li>Best practices</li> <li>Troubleshooting</li> </ul> <p>Use this guide if you prefer task-oriented, practical how\u2011to instructions.</p>"},{"location":"user-guide/#typical-workflow","title":"Typical Workflow","text":"<ol> <li>Initialize: <code>es setup --platform SNOWFLAKE</code> or <code>es setup --platform POSTGRES</code></li> <li>Configure <code>es.ini</code> with credentials and defaults</li> <li>Create tests: add <code>.es.sql</code> files expressing assertions</li> <li>Run: <code>es run -e dev</code></li> <li>Inspect output, and optionally export reports (JUnit XML / Excel)</li> <li>Commit tests to version control and run in CI</li> </ol>"},{"location":"user-guide/#reading-test-output","title":"Reading Test Output","text":"<ul> <li>EchoSphere prints a summary table of discovered tests and a final success/failure status.</li> <li>Pass criteria: a test passes if the executed SQL returns zero rows.</li> <li>Exit codes: 0 for success, non\u2011zero for failure \u2014 ideal for CI gates.</li> <li>Optional exports:</li> <li><code>--junitxml path/to/results.xml</code> to integrate with CI test report viewers</li> <li><code>--export-failures path/to/failures.xlsx</code> to capture failing row details (captures up to 1000 rows per failed test, including column headers)</li> </ul>"},{"location":"user-guide/#environments","title":"Environments","text":"<p>Set your active environment via: - CLI: <code>es run --environment dev</code> - Environment variable: <code>ES_ENV_NAME=dev es run</code></p> <p>See Reference \u2192 Configuration for the <code>es.ini</code> format and fields.</p>"},{"location":"user-guide/using_qualified_names/","title":"Using Fully-Qualified Table Names in SQL Test Files","text":"<p>When writing SQL test files, it is recommended to avoid using fully-qualified names (e.g., <code>database.schema.table</code>) for tables and other database objects. While it may seem convenient to explicitly define the full path to a database object, it significantly reduces the flexibility and reusability of the tests across different environments.</p>"},{"location":"user-guide/using_qualified_names/#key-reasons-to-avoid-fully-qualified-names","title":"Key Reasons to Avoid Fully-Qualified Names","text":"<ol> <li> <p>Environment Portability    Fully-qualified names tie your test to a specific database and schema, making it difficult to execute the same test in different environments. For instance, database and schema names might vary between development, staging, and production (e.g., <code>dev_schema</code>, <code>staging_schema</code>, <code>prod_schema</code>). By relying on default schemas configured dynamically through the connection settings, you can write more portable tests that work seamlessly across environments without requiring changes to the test files.</p> </li> <li> <p>Test Maintenance    Including fully-qualified names increases the effort needed to maintain your test suite. Any changes to database or schema names would require updating every test file where those are referenced, leading to potential errors and unnecessary overhead.</p> </li> <li> <p>Cleaner Test Files    Omitting fully-qualified names makes tests easier to read and manage. Instead of focusing on environment-specific details, the test files can remain concise and focused on the SQL logic being tested, improving readability and reducing the cognitive load for developers.</p> </li> </ol>"},{"location":"user-guide/using_qualified_names/#best-practices","title":"Best Practices","text":"<p>To achieve portability and maintainability:</p> <ul> <li>Configure the schema at the connection level (e.g., using your database configuration in the environment or connection string).</li> <li>Use <code>USE &lt;schema&gt;</code> or equivalent commands in a database setup script to set the schema context for the session before running tests.</li> <li>Avoid hardcoding database and schema names directly in the SQL queries inside test files.</li> </ul> <p>By following these practices, your test files become portable, easy to maintain, and adaptable to any environment, ensuring a more effective testing process.</p>"},{"location":"writing-tests/","title":"Writing Tests","text":"<p>EchoSphere tests are plain SQL files with the <code>.es.sql</code> suffix. A test passes when the query returns zero rows. If the query returns one or more rows, the test fails and the returned rows explain what was wrong.</p>"},{"location":"writing-tests/#file-and-directory-structure","title":"File and Directory Structure","text":"<ul> <li>Use <code>.es.sql</code> as the file extension so EchoSphere can discover tests automatically</li> <li>Organize tests in subdirectories (\"subsuites\") for logical grouping (e.g., <code>smoke/</code>, <code>daily/</code>, <code>integrity/</code>)</li> <li>Reference tests by subsuite using <code>es view</code> (e.g., <code>es view test smoke/orders_total</code>)</li> </ul> <p>Example layout:</p> <pre><code>.es_suite/\n  smoke/\n    orders_total.es.sql\n  integrity/\n    no_duplicate_keys.es.sql\n</code></pre>"},{"location":"writing-tests/#test-authoring-principles","title":"Test Authoring Principles","text":"<ul> <li>Assert via row presence: return a row only when a rule is violated</li> <li>Keep tests focused and isolated \u2014 one assertion per file is easier to triage</li> <li>Prefer deterministic logic; avoid nondeterministic functions unless controlled</li> <li>Keep queries performant; filter early and leverage indexes/partitions where applicable</li> </ul>"},{"location":"writing-tests/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use descriptive, intent\u2011revealing names: <code>no_null_customer_ids.es.sql</code></li> <li>Consider a prefix for categories or ownership if helpful</li> </ul>"},{"location":"writing-tests/#example-tests","title":"Example Tests","text":"<p>Validate a specific aggregation is as expected:</p> <pre><code>-- orders_total.es.sql\nSELECT *\nFROM (\n  SELECT SUM(O_TOTALPRICE) AS total\n  FROM ORDERS\n  WHERE O_ORDERDATE = '1995-02-19'\n)\nWHERE total &lt;&gt; 944870465.07;\n</code></pre> <p>Check for unexpected NULLs:</p> <pre><code>-- no_null_customer_ids.es.sql\nSELECT CUSTOMER_ID\nFROM CUSTOMERS\nWHERE CUSTOMER_ID IS NULL;\n</code></pre> <p>Detect duplicate business keys:</p> <pre><code>-- no_duplicate_orders.es.sql\nSELECT ORDER_ID\nFROM ORDERS\nGROUP BY ORDER_ID\nHAVING COUNT(*) &gt; 1;\n</code></pre>"},{"location":"writing-tests/#best-practices","title":"Best Practices","text":"<ul> <li>Avoid using fully qualified table paths across environments. Rely on environment configuration (database/schema) via <code>es.ini</code>.</li> <li>Keep tests small and composable \u2014 split overly complex checks into multiple tests.</li> <li>Use comments to explain the business rule and failure rationale at the top of each test.</li> <li>For long\u2011running queries, consider limiting the scope (e.g., date partitions) or creating summarized tables upstream.</li> </ul>"},{"location":"writing-tests/#running-and-inspecting","title":"Running and Inspecting","text":"<p>Run all tests:</p> <pre><code>es run -e dev\n</code></pre> <p>List discovered tests:</p> <pre><code>es view tests --all\n</code></pre> <p>Print the SQL of a specific test:</p> <pre><code>es view test integrity/no_duplicate_orders\n</code></pre>"}]}